{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projekt1_raw",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yolRxR2u1lCy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "2d73f0ce-998c-4720-e8f7-2e1d839c9df0"
      },
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# TODO: versions of libraries that will be used:\n",
        "#  Python 3.6.10\n",
        "#  numpy 1.18.3\n",
        "#  scikit-learn 0.22.2.post1\n",
        "#  opencv-python 4.2.0.34\n",
        "\n",
        "\n",
        "def load_dataset(dataset_dir_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    x, y = [], []\n",
        "    for i, class_dir in enumerate(sorted(os.listdir(dataset_dir_path))):   # Sortowanie klas alfabetycznie\n",
        "        class_dir_path = os.path.join(dataset_dir_path, class_dir)\n",
        "        for file in os.listdir(class_dir_path):\n",
        "            img_file = cv2.imread(os.path.join(class_dir_path, file), cv2.IMREAD_GRAYSCALE)\n",
        "            x.append(img_file)\n",
        "            y.append(i)\n",
        "    return np.asarray(x), np.asarray(y)\n",
        "\n",
        "\n",
        "def descriptor2histogram(descriptor, vocab_model, normalize=True) -> np.ndarray:\n",
        "    features_words = vocab_model.predict(descriptor)\n",
        "    histogram = np.zeros(vocab_model.n_clusters, dtype=np.float32)\n",
        "    unique, counts = np.unique(features_words, return_counts=True)\n",
        "    histogram[unique] += counts\n",
        "    if normalize:\n",
        "        histogram /= histogram.sum()\n",
        "    return histogram\n",
        "\n",
        "\n",
        "def apply_feature_transform(\n",
        "        data: np.ndarray,\n",
        "        feature_detector_descriptor,\n",
        "        vocab_model\n",
        ") -> np.ndarray:\n",
        "    data_transformed = []\n",
        "    for image in data:\n",
        "        keypoints, image_descriptor = feature_detector_descriptor.detectAndCompute(image, None)\n",
        "        bow_features_histogram = descriptor2histogram(image_descriptor, vocab_model)\n",
        "        data_transformed.append(bow_features_histogram)\n",
        "    return np.asarray(data_transformed)\n",
        "\n",
        "\n",
        "def data_processing(x: np.ndarray) -> np.ndarray:\n",
        "    # TODO: add data processing here\n",
        "    for i in range(x.size):\n",
        "        img_file = x[i]\n",
        "        width = 768\n",
        "        high = int(img_file.shape[0]*width/img_file.shape[1])\n",
        "        if width < img_file.shape[1] and high < img_file.shape[0]:\n",
        "           img_file = cv2.resize(img_file, (width, high))\n",
        "        x[i] = img_file\n",
        "    return x\n",
        "\n",
        "\n",
        "def project():\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # TODO: fill the following values\n",
        "    first_name = 'Maciej'\n",
        "    last_name = 'Olejniczak'\n",
        "\n",
        "    x, y = load_dataset('./../../test_data/')\n",
        "    #x, y = load_dataset('/content/drive/My Drive/projekt1/test')\n",
        "    #print(x[2].shape,x.size, y)\n",
        "    x = data_processing(x)\n",
        "    #print(x[2].shape,x.size, y)\n",
        "\n",
        "    # TODO: create a detector/descriptor here. Eg. cv2.AKAZE_create()\n",
        "    feature_detector_descriptor = cv2.AKAZE_create()\n",
        "\n",
        "    # TODO: train a vocabulary model and save it using pickle.dump function\n",
        "    vocab_model = pickle.load(open(f'./vocab_model.p', 'rb'))\n",
        "    #vocab_model = pickle.load(open(f'/content/drive/My Drive/projekt1/vocab_model.p', 'rb'))\n",
        "    \n",
        "    x_transformed = apply_feature_transform(x, feature_detector_descriptor, vocab_model)\n",
        "    \n",
        "    # TODO: train a classifier and save it using pickle.dump function\n",
        "    clf = pickle.load(open(f'./clf.p', 'rb'))\n",
        "    #clf = pickle.load(open(f'/content/drive/My Drive/projekt1/clf.p', 'rb'))\n",
        "\n",
        "    score = clf.score(x_transformed, y)\n",
        "    print(f'{first_name} {last_name} score: {score}')\n",
        "    with open(f'{last_name}_{first_name}_score.json', 'w') as f:\n",
        "        json.dump({'score': score}, f)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    project()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab <class 'sklearn.cluster._kmeans.KMeans'>\n",
            "tansmormajtor <class 'numpy.ndarray'>\n",
            "Maciej Olejniczak score: 0.85\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}